# Enhanced LLM Provider MCP Server

An advanced MCP (Model Context Protocol) server that enables Claude to query multiple LLMs simultaneously for verification, comparison, and diverse perspectives. Eliminates the tedious copy-paste workflow when you need multiple AI opinions on complex problems.

## ‚ú® Features

- **ü§ñ Multiple LLM Support**: Query OpenAI, Anthropic, Google Gemini, and Perplexity
- **‚ö° Parallel Processing**: Query all LLMs simultaneously for instant comparison
- **üîÑ Streaming Support**: Real-time response streaming for supported providers
- **üíæ Intelligent Caching**: Avoid redundant API calls with smart response caching
- **üîÑ Automatic Retry**: Built-in retry logic with exponential backoff
- **üìä Response Analysis**: Compare and analyze differences between LLM responses
- **üìà Usage Tracking**: Monitor token usage and response latency
- **üéØ Model Selection**: Choose specific models from each provider
- **üõ°Ô∏è Error Handling**: Graceful error handling with detailed logging

## üìã Prerequisites

- Node.js >= 20.0.0
- npm or yarn
- Claude Desktop app
- API keys for the LLM providers you want to use

## ‚ö° Quick Start (5Î∂Ñ ÏÑ§Ï†ï)

### 1Îã®Í≥Ñ: Claude Desktop ÏÑ§Ï†ï ÌååÏùº Ïó¥Í∏∞

**Windows:** `%APPDATA%\Claude\claude_desktop_config.json` ÌååÏùºÏùÑ Ïó¥Í±∞ÎÇò ÏÉùÏÑ±ÌïòÏÑ∏Ïöî
**macOS:** `~/Library/Application Support/Claude/claude_desktop_config.json` ÌååÏùºÏùÑ Ïó¥Í±∞ÎÇò ÏÉùÏÑ±ÌïòÏÑ∏Ïöî

### 2Îã®Í≥Ñ: ÏÑ§Ï†ï Ï∂îÍ∞Ä

ÌååÏùºÏóê Îã§Ïùå ÎÇ¥Ïö©ÏùÑ Î≥µÏÇ¨ Î∂ôÏó¨ÎÑ£Í∏∞ÌïòÏÑ∏Ïöî:

```json
{
  "mcpServers": {
    "llm-provider": {
      "command": "npx",
      "args": ["@wnsgur1595/llm-provider-mcp"],
      "env": {
        "OPENAI_API_KEY": "Ïó¨Í∏∞Ïóê-ÎãπÏã†Ïùò-openai-ÌÇ§-ÏûÖÎ†•",
        "ANTHROPIC_API_KEY": "Ïó¨Í∏∞Ïóê-ÎãπÏã†Ïùò-anthropic-ÌÇ§-ÏûÖÎ†•"
      }
    }
  }
}
```

### 3Îã®Í≥Ñ: API ÌÇ§ ÏûÖÎ†•

- OpenAI API ÌÇ§Î•º https://platform.openai.com/api-keys ÏóêÏÑú Î∞úÍ∏âÎ∞õÏïÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî
- Anthropic API ÌÇ§Î•º https://console.anthropic.com/keys ÏóêÏÑú Î∞úÍ∏âÎ∞õÏïÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî
- ÌïÑÏöîÌïòÏßÄ ÏïäÏùÄ ÌÇ§Îäî Ï§ÑÏùÑ ÏÇ≠Ï†úÌïòÏÑ∏Ïöî

### 4Îã®Í≥Ñ: Claude Desktop Ïû¨ÏãúÏûë

Claude DesktopÏùÑ ÏôÑÏ†ÑÌûà Ï¢ÖÎ£åÌïòÍ≥† Îã§Ïãú ÏãúÏûëÌïòÏÑ∏Ïöî.

### 5Îã®Í≥Ñ: ÏÇ¨Ïö© ÏãúÏûë!

ClaudeÏóêÏÑú Îã§ÏùåÍ≥º Í∞ôÏù¥ ÎßêÌï¥Î≥¥ÏÑ∏Ïöî:
- "GPTÏóêÍ≤å ÌååÏù¥Ïç¨ ÌïôÏäµ Î∞©Î≤ïÏùÑ Î¨ºÏñ¥Î¥ê"
- "Î™®Îì† LLMÏóêÍ≤å Ïù¥ ÏΩîÎìúÎ•º Í∞úÏÑ†ÌïòÎäî Î∞©Î≤ïÏùÑ Î¨ºÏñ¥Î¥ê"

### üîß Î¨∏Ï†ú Ìï¥Í≤∞

ToolsÍ∞Ä Î≥¥Ïù¥ÏßÄ ÏïäÎÇòÏöî? ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú Îã§Ïùå Î™ÖÎ†πÏñ¥Î°ú ÏÑ§Ï†ïÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî:

```bash
npx @wnsgur1595/llm-provider-mcp --env-check
```

Ïù¥ Î™ÖÎ†πÏñ¥Îäî API ÌÇ§ ÏÑ§Ï†ï ÏÉÅÌÉúÎ•º Î≥¥Ïó¨Ï§çÎãàÎã§.

---

## üîå Îã§Î•∏ MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏôÄ ÏÇ¨Ïö©ÌïòÍ∏∞

Ïù¥ MCP ÏÑúÎ≤ÑÎäî Claude Desktop Ïô∏ÏóêÎèÑ Îã§ÏñëÌïú MCP Ìò∏Ìôò ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏóêÏÑú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.

### 1. MCP Inspector (Í∞úÎ∞ú/ÌÖåÏä§Ìä∏Ïö©)

Ïõπ Î∏åÎùºÏö∞Ï†ÄÏóêÏÑú MCP ÏÑúÎ≤ÑÎ•º ÏßÅÏ†ë ÌÖåÏä§Ìä∏ÌïòÍ≥† ÎîîÎ≤ÑÍπÖÌï† Ïàò ÏûàÏäµÎãàÎã§:

```bash
# API ÌÇ§Î•º ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú ÏÑ§Ï†ï
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"

# MCP Inspector Ïã§Ìñâ
npx @modelcontextprotocol/inspector npx @wnsgur1595/llm-provider-mcp
```

Î∏åÎùºÏö∞Ï†ÄÏóêÏÑú http://localhost:5173 ÏúºÎ°ú Ï†ëÏÜçÌïòÏó¨ toolsÎ•º ÏßÅÏ†ë ÌÖåÏä§Ìä∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.

### 2. VS Code Extensions

MCPÎ•º ÏßÄÏõêÌïòÎäî VS Code ÌôïÏû•ÌîÑÎ°úÍ∑∏Îû®ÏóêÏÑú ÏÇ¨Ïö©:

```json
// VS Code settings.json
{
  "mcp.servers": {
    "llm-provider": {
      "command": "npx",
      "args": ["@wnsgur1595/llm-provider-mcp"],
      "env": {
        "OPENAI_API_KEY": "your-openai-key"
      }
    }
  }
}
```

### 3. Ïª§Ïä§ÌÖÄ MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏

MCP SDKÎ•º ÏÇ¨Ïö©Ìï¥ÏÑú ÏßÅÏ†ë Í∞úÎ∞úÌïú Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏóêÏÑú Ïó∞Í≤∞:

```typescript
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";

const transport = new StdioClientTransport({
  command: "npx",
  args: ["@wnsgur1595/llm-provider-mcp"],
  env: {
    OPENAI_API_KEY: process.env.OPENAI_API_KEY
  }
});

const client = new Client({
  name: "my-app",
  version: "1.0.0"
}, {
  capabilities: {}
});

await client.connect(transport);
```

### 4. ÏßÅÏ†ë CLI Ïã§Ìñâ

ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú ÏÑúÎ≤ÑÎ•º ÏßÅÏ†ë Ïã§ÌñâÌïòÏó¨ stdio Î™®ÎìúÎ°ú ÌÜµÏã†:

```bash
# ÌôòÍ≤ΩÎ≥ÄÏàò ÏÑ§Ï†ï
export OPENAI_API_KEY="your-key"
export ANTHROPIC_API_KEY="your-key"

# ÏÑúÎ≤Ñ Ïã§Ìñâ (stdio Î™®Îìú)
npx @wnsgur1595/llm-provider-mcp

# ÎòêÎäî ÌôòÍ≤Ω Ï≤¥ÌÅ¨Îßå
npx @wnsgur1595/llm-provider-mcp --env-check
```

### 5. Docker Ïª®ÌÖåÏù¥ÎÑà

DockerÎ•º ÏÇ¨Ïö©Ìïú Í≤©Î¶¨Îêú ÌôòÍ≤ΩÏóêÏÑú Ïã§Ìñâ:

```dockerfile
FROM node:20-alpine

# Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò
RUN npm install -g @wnsgur1595/llm-provider-mcp

# ÌôòÍ≤ΩÎ≥ÄÏàò ÏÑ§Ï†ï
ENV OPENAI_API_KEY=""
ENV ANTHROPIC_API_KEY=""

# ÏÑúÎ≤Ñ Ïã§Ìñâ
CMD ["llm-provider-mcp"]
```

```bash
# Docker Ïã§Ìñâ
docker run -e OPENAI_API_KEY="your-key" -e ANTHROPIC_API_KEY="your-key" your-image
```

### 6. Îã§Î•∏ AI ÌîåÎû´ÌèºÍ≥º ÌÜµÌï©

- **Continue**: VS Code AI ÏΩîÎî© Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏
- **Aider**: AI ÌéòÏñ¥ ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç ÎèÑÍµ¨
- **Í∏∞ÌÉÄ MCP Ìò∏Ìôò ÎèÑÍµ¨Îì§**

> **üí° ÌåÅ**: MCP InspectorÎäî ÏÉàÎ°úÏö¥ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Í∞úÎ∞úÏù¥ÎÇò ÎîîÎ≤ÑÍπÖ Ïãú Îß§Ïö∞ Ïú†Ïö©Ìï©ÎãàÎã§. toolsÏùò Ïä§ÌÇ§ÎßàÏôÄ ÏùëÎãµÏùÑ Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.

---

## üöÄ ÏÉÅÏÑ∏ ÏÑ§Ïπò Í∞ÄÏù¥Îìú

### Option 1: Use with npx (Recommended)

No installation required! Just configure Claude Desktop to use:

```json
{
  "mcpServers": {
    "llm-provider": {
      "command": "npx",
      "args": ["@wnsgur1595/llm-provider-mcp"],
      "env": { ... }
    }
  }
}
```

### Option 2: Install globally

```bash
npm install -g @wnsgur1595/llm-provider-mcp
```

### Option 3: Build from Source

```bash
# Clone the repository
git clone https://github.com/wnsgur1595/llm-provider.git
cd llm-provider

# Install dependencies
npm install

# Build the project
npm run build
```

## üîß Configuration

### 1. Set up API Keys

You can configure API keys in **two ways** (in order of priority):

#### Option A: Claude Desktop Configuration (Recommended)
Configure directly in Claude Desktop config:

**On macOS:** `~/Library/Application Support/Claude/claude_desktop_config.json`
**On Windows:** `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "llm-provider": {
      "command": "npx",
      "args": ["@wnsgur1595/llm-provider-mcp"],
      "env": {
        "OPENAI_API_KEY": "sk-your-openai-key",
        "ANTHROPIC_API_KEY": "sk-ant-your-anthropic-key",
        "GOOGLE_API_KEY": "your-google-api-key",
        "PERPLEXITY_API_KEY": "pplx-your-perplexity-key",
        
        "OPENAI_MODEL": "gpt-5-mini",
        "ANTHROPIC_MODEL": "claude-sonnet-4",
        "GOOGLE_MODEL": "gemini-2.5-flash",
        "PERPLEXITY_MODEL": "sonar-pro",
        
        "LOG_LEVEL": "info"
      }
    }
  }
}
```

**Alternative (if installed globally):**
```json
{
  "mcpServers": {
    "llm-provider": {
      "command": "@wnsgur1595/llm-provider-mcp",
      "env": {
        "OPENAI_API_KEY": "sk-your-openai-key",
        "ANTHROPIC_API_KEY": "sk-ant-your-anthropic-key"
      }
    }
  }
}
```

#### Option B: System Environment Variables
Set environment variables on your system:

**Windows:**
```cmd
# Set user environment variables
setx OPENAI_API_KEY "sk-your-openai-key"
setx ANTHROPIC_API_KEY "sk-ant-your-anthropic-key"
setx GOOGLE_API_KEY "your-google-api-key"
setx PERPLEXITY_API_KEY "pplx-your-perplexity-key"
```

**macOS/Linux:**
```bash
# Add to ~/.bashrc, ~/.zshrc, or ~/.profile
export OPENAI_API_KEY="sk-your-openai-key"
export ANTHROPIC_API_KEY="sk-ant-your-anthropic-key"
export GOOGLE_API_KEY="your-google-api-key"
export PERPLEXITY_API_KEY="pplx-your-perplexity-key"
```

> **üö® Important:** Only providers with valid API keys will be initialized. Providers without API keys will be automatically skipped.

### 2. Restart Claude Desktop

After saving the configuration, restart Claude Desktop to load the MCP server.

## üí¨ Usage Examples

Once configured, you can use natural language commands in Claude:

### Query a Specific LLM
```
"Ask GPT-4 what it thinks about quantum computing"
"Get Gemini's take on this code optimization"
"What does Perplexity say about recent AI developments?"
```

### Query All LLMs
```
"Ask all available LLMs about the best practices for microservices"
"Get everyone's opinion on this technical architecture"
"Cross-check this solution with all models"
```

### Compare Responses
```
"Compare what different LLMs think about this problem"
"Analyze the consensus on this approach"
"Show me how the models differ in their explanations"
```

### Advanced Usage
```
"Ask GPT-4 with temperature 0.9 about creative solutions"
"Query all models with a specific system prompt"
"Use Claude Opus model specifically for this question"
```

## üõ†Ô∏è Available Tools

The server provides these MCP tools:

- `ask_openai` - Query OpenAI models
- `ask_anthropic` - Query Anthropic Claude models
- `ask_google` - Query Google Gemini models
- `ask_perplexity` - Query Perplexity models
- `ask_all_llms` - Query all configured LLMs in parallel
- `compare_llm_responses` - Get structured comparison of responses

## üìä Supported Models

### OpenAI (2025 Latest)
- **GPT-5** (Default) - Latest flagship model with expert-level intelligence
- **GPT-5 Mini** - Fast and cost-effective version
- **GPT-5 Nano** - Ultra-fast and most affordable
- **GPT-5 Chat Latest** - ChatGPT optimized version
- GPT-4o (Optimized) - Legacy support
- GPT-4o Mini - Legacy support

### Anthropic (2025 Latest)
- **Claude Sonnet 4** (Default) - Superior coding and reasoning with precise instruction following
- **Claude Opus 4.1** - Most powerful model with world-class coding abilities (premium pricing)
- **Claude 3.7 Sonnet** - Hybrid reasoning model with step-by-step thinking
- Claude 3.5 Sonnet - Legacy support
- Claude 3.5 Haiku - Legacy support

### Google (2025 Latest)
- **Gemini 2.5 Pro** (Default) - State-of-the-art thinking model with 1M+ context window
- **Gemini 2.5 Flash** - Fast version with thinking capabilities
- Gemini 1.5 Pro - Legacy support
- Gemini 1.5 Flash - Legacy support

### Perplexity (2025 Latest)
- **Sonar** (Default) - Latest model based on Llama 3.3 70B with real-time search
- **Sonar Pro** - Enhanced version for complex questions and detailed answers
- Llama 3.1 Sonar Large Online - Legacy support
- Llama 3.1 Sonar Small Online - Legacy support

## üîç Debugging

### Enable Debug Logging

Set the log level in your `.env` file:

```env
LOG_LEVEL=debug
```

### Check Logs

Logs are written to:
- Console output (colored for readability)
- `llm-provider.log` file (JSON format)

### Common Issues

1. **"No LLM providers configured"**
   - Ensure you have at least one API key set in your `.env` file

2. **"Rate limit exceeded"**
   - The server includes automatic retry with exponential backoff
   - Consider implementing request queuing for high-volume usage

3. **"Tool not showing in Claude"**
   - Restart Claude Desktop after configuration changes
   - Check the logs for initialization errors
   - Verify the path to the server is correct

## üß™ Development

### Running in Development Mode

```bash
npm run dev
```

### Running Tests

```bash
npm test
npm run test:watch  # Watch mode
```

### Linting and Formatting

```bash
npm run lint
npm run format
```

## üèóÔ∏è Project Structure

```
llm-provider-mcp/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts           # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ server.ts          # MCP server setup
‚îÇ   ‚îú‚îÄ‚îÄ config/            # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ providers/         # LLM provider implementations
‚îÇ   ‚îú‚îÄ‚îÄ tools/             # MCP tool definitions
‚îÇ   ‚îú‚îÄ‚îÄ utils/             # Utility functions
‚îÇ   ‚îî‚îÄ‚îÄ types/             # TypeScript type definitions
‚îú‚îÄ‚îÄ tests/                 # Test files
‚îú‚îÄ‚îÄ docs/                  # Documentation
‚îú‚îÄ‚îÄ .env.example          # Environment variables template
‚îú‚îÄ‚îÄ package.json          # Dependencies and scripts
‚îî‚îÄ‚îÄ tsconfig.json         # TypeScript configuration
```

## üìà Performance Optimization

- **Caching**: Responses are cached for 1 hour by default (configurable)
- **Parallel Queries**: All LLMs are queried simultaneously when using `ask_all_llms`
- **Streaming**: Supported providers stream responses for better UX
- **Retry Logic**: Automatic retry with exponential backoff for transient failures

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

MIT License - see LICENSE file for details

```
MIT License

Copyright (c) 2024 LLM Handoff MCP

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## üôè Acknowledgments

- Built on the [Model Context Protocol SDK](https://github.com/modelcontextprotocol/typescript-sdk)
- Inspired by the original [llm-provider-from-claude-mcp](https://github.com/keyhoffman/llm-provider-from-claude-mcp) project
- Thanks to all LLM providers for their APIs

## üìû Support

- **Issues**: Open an issue on [GitHub](https://github.com/wnsgur1595/llm-provider/issues)
- **Discussions**: Join our [Discord community](https://discord.gg/yourinvite)
- **Documentation**: Check the [docs](./docs) folder for detailed guides
- **Email**: support@example.com

## üö¶ Status

![Build Status](https://img.shields.io/badge/build-passing-brightgreen)
![Version](https://img.shields.io/badge/version-2.0.0-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Node Version](https://img.shields.io/badge/node-%3E%3D20.0.0-brightgreen)

---

<div align="center">
Made with ‚ù§Ô∏è for the AI community
</div>